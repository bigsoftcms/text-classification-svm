{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Data wrangling \n",
    "## Assignment 4 - Text Preprocessing\n",
    "\n",
    "\n",
    "\n",
    "### Author: Ramprasath Karunakaran\n",
    "\n",
    "ID: 26994437\n",
    "\n",
    "Date written: 18/10/2016\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Program: Python 2.7.12 and Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#checking python version\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing libraries that will be used in this report\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Parsing the XML files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given patent is completely in the XML format. From these XML, we have to extract the following details:\n",
    "> - Patent ID\n",
    "- Section ID\n",
    "- Abstract\n",
    "- Description\n",
    "- Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#folder which has all the patent files\n",
    "patents_folder=\"./100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to create a dictionary for each patent using object instantiation method\n",
    "class Patent(object):\n",
    "    def __init__(self, documentID, sectionID, abstract, description, claims):\n",
    "        self.documentID = documentID\n",
    "        self.sectionID = sectionID\n",
    "        self.abstract = abstract\n",
    "        self.description = description\n",
    "        self.claims = claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All the functions used below are derived from the three chapters of Fundamentals of Text Preprocessing given in Alexandria materials.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given XML files can be parsed using the Element Tree in Python's XML library. Hence, all the details will be stored in a tree structure where each XML tag serves as a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#empty list to store all the patents\n",
    "patentList =[]\n",
    "#iterating through all the files and subfolders in the folder\n",
    "for root,subfolders,files in os.walk(patents_folder):\n",
    "    for file_name in files:\n",
    "        #combining all the file path which will be fed to element tree for parsing\n",
    "        file_path  = os.path.join(root, file_name)\n",
    "        # to parse only xml files\n",
    "        if file_path.endswith('XML'):\n",
    "            # converting the file to a xml tree\n",
    "            tree = ET.parse(file_path)\n",
    "            # locating the required nodes and storing them in variables\n",
    "            documentID = tree.find('.//doc-number').text\n",
    "            sectionID = tree.find('.//section').text\n",
    "            abst = tree.find('.//abstract')\n",
    "            abstract = ET.tostring(abst, method=\"text\",encoding='UTF-8') #XML uses UTF-8 codec\n",
    "            desc = tree.find('.//description')\n",
    "            description = ET.tostring(desc, method=\"text\",encoding='UTF-8')\n",
    "            clm = tree.find('.//claims')\n",
    "            claims = ET.tostring(clm, method=\"text\",encoding='UTF-8')\n",
    "            # create a patent dictionary by creating a new object\n",
    "            patent = Patent(documentID, sectionID, abstract, description,claims)\n",
    "            # appending the patent to the list\n",
    "            patentList.append(patent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the XML files are parsed using trees. All the patentIDs and the section IDs are stored in a text file called 'section_labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extracting the document ID and section ID alone from the patentList\n",
    "labelInfo = dict([(patent.documentID, patent.sectionID) for patent in patentList])\n",
    "# opening a new csv file in the write mode\n",
    "labelFile = csv.writer(open(\"section_labels.txt\",\"w\"))\n",
    "for key, value in labelInfo.items():\n",
    "    # reading every key value and writing them into a file\n",
    "    labelFile.writerow([key, value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Tokenisation of Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the patent list created above, the abstract, description and claims sections are chosen to be tokenised. Hence we create a dictionary with patent IDs and their respective text details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extracting text of patents with document IDs\n",
    "patentText = dict([ (patent.documentID ,\",\".join((patent.abstract,patent.description,patent.claims))) for patent in patentList])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the given files are patent files which are properly formatted using XML. Hence, there were no special characters involved in those documents. If there were some special characters, we will be in need of regular expressions to customise the process of tokenisation. So, we use the tokeniser library available in Natural Language ToolKit(NLTK) which will tokenise the entire dictionary 'patentText' choosing only the alphabet letters in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_sent(sent):\n",
    "    \"\"\"\n",
    "    The function tokenizes a sentence, and return a list of words that only contain alphabet \n",
    "    letters.\n",
    "    \"\"\"\n",
    "    #lower() converts the words to lowercase which will avoid confusion while creating a dictionary of vocabulary\n",
    "    return [word for word in nltk.word_tokenize(sent.lower()) if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dictionary to store patent ids and their corresponding set of tokens\n",
    "tokenized_sents = {} \n",
    "# iterating through the patent text\n",
    "for keys in patentText.iterkeys():\n",
    "    #decode function is to support the ascii codec in python and all tokens are in UTF-8 encoding \n",
    "    tokenized_sents[keys] = tokenize_sent(patentText[keys].decode('utf-8')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Hence, all the patent IDs and their respective tokens are stored in a dictionary for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we try to find the most common tokens in the data using the FreqDist function in NLTK probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.probability import *\n",
    "\n",
    "def word_concat(dsd):\n",
    "    \"\"\"\n",
    "    concatenate all the words stored in the values of a given dictionary. Each value is a list\n",
    "    of tokenized sentences.\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    # iterates all patents and store the tokens in a single list.\n",
    "    for sent in dsd.values():\n",
    "            all_words += sent\n",
    "    print \"tokens:\", len(all_words) # total number of tokens \n",
    "    print \"types:\", len(set(all_words)) # total number of types\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#listing the common words and their number of occurences\n",
    "freq_dist = FreqDist(word_concat(tokenized_sents))\n",
    "freq_dist.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above frequencies, it is evident that stop words like 'the','of' are going to affect our process of preprocessing since they do not provide much value to the processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we try to remove the stopwords before finding bigrams and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import set of 127 stop words from NLTK corpus\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Remove the above stop words from the tokenised sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for keys in tokenized_sents.iterkeys():\n",
    "    filtered_sents = [token for token in tokenized_sents[keys] if token not in stopwords_list ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we import another set of Stopwords from [Kevin Bouge's website](https://sites.google.com/site/kevinbouge/stopwords-lists) and store it in a separate text file and remove them from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#list to store the stop words\n",
    "stopwords_list_570 = []\n",
    "# all the stopwords are stored in a text file\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords_list_570 = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing the 570 stop words\n",
    "filtered_sents = [w for w in tokenized_sents[keys] if w.lower() not in stopwords_list_570]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, we try to list the most common words from the tokens using the same function as above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.probability import *\n",
    "freq_dist = FreqDist(filtered_sents)\n",
    "freq_dist.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we try to find bigrams and trigrams using the NLTK in-built methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#finding bigrams and trigrams\n",
    "my_bigrams = nltk.bigrams(filtered_sents)\n",
    "my_trigrams = nltk.trigrams(filtered_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#storing bigrams and trigrams in a list\n",
    "list_bigrams=list(my_bigrams)\n",
    "list_trigrams=list(my_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trying to find the frequency of bigrams and trigrams\n",
    "common_trigrams = FreqDist(list_trigrams)\n",
    "common_bigrams = FreqDist(list_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, we extract the top 200 bigrams and top 100 trigrams to retokenise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the top 200 bigram and 100 trigram based on the frequency distribution\n",
    "trigrams =[item[0] for item in common_trigrams.most_common(100)]\n",
    "bigrams = [item[0] for item in common_bigrams.most_common(200)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we retokenise the given text by adding the bigrams and trigrams to our set of tokens using Multiword Tokeniser in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge the list of trigram and bigram\n",
    "multiwordlist = []\n",
    "multiwordlist.extend(bigrams)\n",
    "multiwordlist.extend(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "mwe_tokens = {}\n",
    "for keys in tokenized_sents.iterkeys():\n",
    "    mwe_tokenizer = MWETokenizer(multiwordlist)\n",
    "    # applying the trigram and bigram to the patent dict with stop words\n",
    "    mwe_tokens[keys] = mwe_tokenizer.tokenize(tokenized_sents[keys])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, we have a dictionary of patent IDs and their respective tokens(multiwords implemented)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 3 - Vectors\n",
    "To implement vectors, we use the tokens without removing stop words and we generate a list of sentences for each patent by combining all the tokens. These sentences will make it easy for us to fit the data in vectoriser functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list of patent ids\n",
    "sents_ids = []\n",
    "#list of sentences\n",
    "sents_words =[]\n",
    "for key, value in mwe_tokens.items():\n",
    "    sents_ids.append(\"{0}\".format(key))\n",
    "    txt = ' '.join(value) # joining all the tokens to form a sentence\n",
    "    sents_words.append(txt) # appending the sentences to the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce TF-IDF vectors, we use TfidfVectorizer from Scikit library which will fit the sentences we formed above to get a weight factor matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorizer is set to analyse word by word\n",
    "tfidf_vectorizer = TfidfVectorizer(input = 'content', analyzer = 'word')\n",
    "#fitting the sentences into the vectorizer\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(sents_words)\n",
    "tfidf_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We get a matrix with 800 rows as patent ids and 34534 words as columns. And the intersection of each row and column is the weight of the particular word using the TFID vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the set of entire columns as the vocabulary.\n",
    "vocab = tfidf_vectorizer.get_feature_names()\n",
    "vocab_list = [] \n",
    "sub_list = []\n",
    "# combining the patent id, word index and the weight\n",
    "for i, val in enumerate(sents_ids):\n",
    "    for word, weight in zip(vocab, tfidf_vectors.toarray()[i]):\n",
    "        if weight > 0: #weight 0 is of no use\n",
    "            sub_list = [val,list(vocab).index(word),weight]\n",
    "            vocab_list += [sub_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Store all the weights in a separate text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idfFile = csv.writer(open(\"tf_idf_vectors.txt\",\"w\"))\n",
    "for value in vocab_list:\n",
    "    # reading every value and writing them into a file\n",
    "    tf_idfFile.writerow(value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectors and Binary Vectors.\n",
    "To use count vectors, we make use of the CountVectorizer. To find the binary vector, we use count vector weight. If count vector weight is greater than 0, we set the binary vector weight to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(input = 'content',analyzer = \"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fitting the sentences into the vectorizer\n",
    "data_features = vectorizer.fit_transform(sents_words)\n",
    "print data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countvocab = vectorizer.get_feature_names()\n",
    "countvocab_list = [] \n",
    "countsub_list = []\n",
    "binaryvocab_list = []\n",
    "binarysub_list = []\n",
    "\n",
    "#combining the patent id, word index and the weight for count and binary vector\n",
    "for i, val in enumerate(sents_ids):\n",
    "    for word, weight in zip(countvocab, data_features.toarray()[i]):\n",
    "        if weight > 0:\n",
    "            countsub_list = [val,list(countvocab).index(word),weight]\n",
    "            binarysub_list = [val,list(countvocab).index(word),1]\n",
    "            countvocab_list += [countsub_list]\n",
    "            binaryvocab_list += [binarysub_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a separate file for count vectors\n",
    "countVectorsFile = csv.writer(open(\"count_vectors.txt\",\"w\"))\n",
    "for value in countvocab_list:\n",
    "    # reading every key value and writing them into a file\n",
    "    countVectorsFile.writerow(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a separate file for binary vectors\n",
    "with open(\"binary_vectors.txt\", \"wb\") as outfile:\n",
    "    writer = csv.writer(outfile, delimiter = \",\")\n",
    "    #for each dictionary item (key and value) their respective value are written in the file\n",
    "    for val in binaryvocab_list:\n",
    "        writer.writerow(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running SVM classifier without removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python svm_classifier.py count_vectors.txt section_labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC 0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python svm_classifier.py binary_vectors.txt  section_labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC 0.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python svm_classifier.py tf_idf_vectors.txt  section_labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retokenising the multi word token set by removing the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_sents_retoken = {}\n",
    "for keys in mwe_tokens.iterkeys():\n",
    "    filtered_sents_retoken[keys] = [token for token in mwe_tokens[keys] if token not in stopwords_list ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the stop words are removed, we generate a new set of senetence for task 3 above by using the below function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of patent ids\n",
    "sents_ids = []\n",
    "#list of sentences\n",
    "sents_words =[]\n",
    "#using a different set of tokens after removing stop words\n",
    "for key, value in filtered_sents_retoken.items():\n",
    "    sents_ids.append(\"{0}\".format(key))\n",
    "    txt = ' '.join(value) # joining all the tokens to form a sentence\n",
    "    sents_words.append(txt) # appending the sentences to the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running all the vectors again from task 3, different sets of weights are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python svm_classifier.py count_vectors.txt section_labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC 0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python svm_classifier.py binary_vectors.txt  section_labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC 0.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python svm_classifier.py tf_idf_vectors.txt  section_labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC 0.74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When stop words are removed, AUC value increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
